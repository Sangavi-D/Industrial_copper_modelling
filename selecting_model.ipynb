{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INDUSTRIAL COPPER MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SANGAVI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "180 fits failed out of a total of 540.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "180 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\SANGAVI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\SANGAVI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\SANGAVI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\SANGAVI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\SANGAVI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.25766728 0.42126185 0.51066429\n",
      " 0.3362986  0.3134089  0.34344886 0.03216761 0.51703612 0.48737546\n",
      " 0.24263368 0.05649371 0.35042027 0.18351713 0.51343579 0.0197906\n",
      " 0.31837867 0.18182577 0.20246036        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.38800716 0.71964866 0.30294983 0.45943466 0.47621072 0.45858069\n",
      " 0.6702905  0.51205905 0.53672181 0.37829607 0.33640765 0.33185854\n",
      " 0.52966375 0.49990876 0.44030092 0.59923028 0.71770751 0.62373596\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.80211029 0.85094506 0.73824846\n",
      " 0.75798608 0.86169401 0.84572651 0.82265047 0.816971   0.79702748\n",
      " 0.70142732 0.7931566  0.76239774 0.84259339 0.81736014 0.80104812\n",
      " 0.80842973 0.74142706 0.83349271        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.87344258 0.89665926 0.89307257 0.8917115  0.89448223 0.89406028\n",
      " 0.88493223 0.89070604 0.87725193 0.89799747 0.88631703 0.88256367\n",
      " 0.89373993 0.89561495 0.89847318 0.8948613  0.88113638 0.88074948]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
      "Mean squared error: 0.027367974782014855\n",
      "R-squared: 0.916815090264562\n"
     ]
    }
   ],
   "source": [
    "#DecisionTree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df= pd.read_csv(\"copper_preprocessed.csv\")\n",
    "\n",
    "X=df[['quantity tons_log','status','item type','application','thickness_log','width','country','customer','product_ref']]\n",
    "y=df['selling_price_log']\n",
    "# encoding categorical variables\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(X[['item type']])\n",
    "X_ohe = ohe.fit_transform(X[['item type']]).toarray()\n",
    "ohe2 = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe2.fit(X[['status']])\n",
    "X_be = ohe2.fit_transform(X[['status']]).toarray()\n",
    "# independent features after encoding\n",
    "X = np.concatenate((X[['quantity tons_log', 'application', 'thickness_log', 'width','country','customer','product_ref']].values, X_ohe, X_be), axis=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# test and train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "# decision tree\n",
    "dtr = DecisionTreeRegressor()\n",
    "# hyperparameters\n",
    "param_grid = {'max_depth': [2, 5, 10, 20],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'max_features': ['auto', 'sqrt', 'log2']}\n",
    "# gridsearchcv\n",
    "grid_search = GridSearchCV(estimator=dtr, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "# evalution metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('Mean squared error:', mse)\n",
    "print('R-squared:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted selling price: [1103.73085412]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SANGAVI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\SANGAVI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ['quantity tons_log', 'application', 'thickness_log', 'width','country','customer','product_ref']].values, X_ohe, X_be\n",
    "new_sample = np.array([[np.log(40), 10, np.log(250), 0, 28,30202938,1670798778,'PL','Won']])\n",
    "new_sample_ohe = ohe.transform(new_sample[:, [7]]).toarray()\n",
    "new_sample_be = ohe2.transform(new_sample[:, [8]]).toarray()\n",
    "new_sample = np.concatenate((new_sample[:, [0,1,2, 3, 4, 5, 6,]], new_sample_ohe, new_sample_be), axis=1)\n",
    "new_sample1 = scaler.transform(new_sample)\n",
    "new_pred = best_model.predict(new_sample1)\n",
    "print('Predicted selling price:', np.exp(new_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"copper_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'item_date', 'quantity tons', 'customer', 'country',\n",
       "       'status', 'item type', 'application', 'thickness', 'width',\n",
       "       'material_ref', 'product_ref', 'delivery date', 'selling_price',\n",
       "       'selling_price_log', 'quantity tons_log', 'thickness_log'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.019899021323904264\n",
      "R-squared: 0.9395169607602705\n",
      "Model, scaler, and encoder saved for future predictions!\n"
     ]
    }
   ],
   "source": [
    "#Randomforest Final\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df = pd.read_csv(\"copper_preprocessed.csv\")\n",
    "\n",
    "X = df[\n",
    "    [\n",
    "        \"quantity tons_log\",\n",
    "        \"status\",\n",
    "        \"item type\",\n",
    "        \"application\",\n",
    "        \"thickness_log\",\n",
    "        \"width\",\n",
    "        \"country\",\n",
    "        \"customer\",\n",
    "        \"product_ref\",\n",
    "    ]\n",
    "]\n",
    "y = df[\"selling_price_log\"]\n",
    "\n",
    "# Encode categorical features\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohe.fit(X[[\"item type\", \"status\"]])  # Encode both 'item type' and 'status'\n",
    "X_ohe = ohe.transform(X[[\"item type\", \"status\"]]).toarray()\n",
    "\n",
    "# Combine numerical and encoded features\n",
    "X = np.concatenate(\n",
    "    (\n",
    "        X[\n",
    "            [\n",
    "                \"quantity tons_log\",\n",
    "                \"application\",\n",
    "                \"thickness_log\",\n",
    "                \"width\",\n",
    "                \"country\",\n",
    "                \"customer\",\n",
    "                \"product_ref\",\n",
    "            ]\n",
    "        ].values,\n",
    "        X_ohe,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Random Forest Regressor (using default hyperparameters)\n",
    "rfr = RandomForestRegressor()\n",
    "\n",
    "# Train the model\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = rfr.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Save model and scaler for future predictions\n",
    "with open(\"random_forest_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rfr, f)\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open(\"ohe.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ohe, f)\n",
    "\n",
    "print(\"Model, scaler, and encoder saved for future predictions!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.024104700756109317\n",
      "R-squared: 0.9267338057504203\n"
     ]
    }
   ],
   "source": [
    "#ExtraTreesRegressor 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df = pd.read_csv(\"copper_preprocessed.csv\")\n",
    "\n",
    "X = df[\n",
    "    [\n",
    "        \"quantity tons_log\",\n",
    "        \"status\",\n",
    "        \"item type\",\n",
    "        \"application\",\n",
    "        \"thickness_log\",\n",
    "        \"width\",\n",
    "        \"country\",\n",
    "        \"customer\",\n",
    "        \"product_ref\",\n",
    "    ]\n",
    "]\n",
    "y = df[\"selling_price_log\"]\n",
    "\n",
    "# Encode categorical features\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohe.fit(X[[\"item type\", \"status\"]])  # Encode both 'item type' and 'status'\n",
    "X_ohe = ohe.transform(X[[\"item type\", \"status\"]]).toarray()\n",
    "\n",
    "# Combine numerical and encoded features\n",
    "X = np.concatenate(\n",
    "    (\n",
    "        X[\n",
    "            [\n",
    "                \"quantity tons_log\",\n",
    "                \"application\",\n",
    "                \"thickness_log\",\n",
    "                \"width\",\n",
    "                \"country\",\n",
    "                \"customer\",\n",
    "                \"product_ref\",\n",
    "            ]\n",
    "        ].values,\n",
    "        X_ohe,\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# ExtraTreesRegressor (using default hyperparameters)\n",
    "etr = ExtraTreesRegressor()\n",
    "\n",
    "# Train the model\n",
    "etr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = etr.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\sangavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\sangavi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.13.0)\n",
      "Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB 435.7 kB/s eta 0:03:49\n",
      "   ---------------------------------------- 0.1/99.8 MB 465.5 kB/s eta 0:03:35\n",
      "   ---------------------------------------- 0.1/99.8 MB 726.2 kB/s eta 0:02:18\n",
      "   ---------------------------------------- 0.2/99.8 MB 1.2 MB/s eta 0:01:27\n",
      "   ---------------------------------------- 0.5/99.8 MB 1.8 MB/s eta 0:00:54\n",
      "   ---------------------------------------- 0.6/99.8 MB 2.0 MB/s eta 0:00:49\n",
      "   ---------------------------------------- 1.0/99.8 MB 3.2 MB/s eta 0:00:32\n",
      "    --------------------------------------- 1.3/99.8 MB 3.4 MB/s eta 0:00:30\n",
      "    --------------------------------------- 2.2/99.8 MB 5.2 MB/s eta 0:00:19\n",
      "    --------------------------------------- 2.2/99.8 MB 5.2 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 2.8/99.8 MB 5.4 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 3.5/99.8 MB 6.3 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 4.1/99.8 MB 6.7 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 4.8/99.8 MB 7.7 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 5.6/99.8 MB 8.1 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 6.5/99.8 MB 8.8 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 7.6/99.8 MB 9.7 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 7.9/99.8 MB 9.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 9.2/99.8 MB 10.4 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 9.4/99.8 MB 10.0 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 10.3/99.8 MB 10.9 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 11.2/99.8 MB 14.6 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 11.5/99.8 MB 14.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 12.2/99.8 MB 14.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 12.3/99.8 MB 13.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 12.3/99.8 MB 13.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 14.5/99.8 MB 16.0 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 14.7/99.8 MB 15.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 15.3/99.8 MB 15.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 15.3/99.8 MB 15.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 16.3/99.8 MB 14.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 16.3/99.8 MB 14.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 17.0/99.8 MB 13.9 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 17.8/99.8 MB 13.1 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 18.3/99.8 MB 13.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 18.3/99.8 MB 13.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 19.0/99.8 MB 12.1 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 19.3/99.8 MB 11.7 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 19.5/99.8 MB 11.7 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 19.8/99.8 MB 11.7 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 20.1/99.8 MB 11.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 20.4/99.8 MB 10.7 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 20.7/99.8 MB 10.4 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 21.0/99.8 MB 10.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 21.3/99.8 MB 9.9 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 21.5/99.8 MB 9.6 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 21.9/99.8 MB 9.5 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 22.2/99.8 MB 9.2 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 22.6/99.8 MB 9.5 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 22.9/99.8 MB 9.8 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 23.1/99.8 MB 9.4 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 23.5/99.8 MB 9.0 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 23.7/99.8 MB 8.5 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 24.1/99.8 MB 8.3 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 24.5/99.8 MB 8.0 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 24.7/99.8 MB 7.8 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 25.1/99.8 MB 7.7 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 25.4/99.8 MB 7.6 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 25.7/99.8 MB 7.9 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 26.0/99.8 MB 7.6 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 26.5/99.8 MB 7.4 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 26.8/99.8 MB 7.4 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 27.1/99.8 MB 7.3 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 27.5/99.8 MB 7.4 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 27.9/99.8 MB 7.1 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 28.1/99.8 MB 7.0 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 28.3/99.8 MB 7.0 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 28.5/99.8 MB 7.1 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 28.6/99.8 MB 7.0 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 29.1/99.8 MB 6.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 29.4/99.8 MB 6.7 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 29.5/99.8 MB 6.6 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 29.8/99.8 MB 6.6 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 30.0/99.8 MB 6.5 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 30.4/99.8 MB 6.5 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 30.6/99.8 MB 6.5 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 30.8/99.8 MB 6.4 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 31.1/99.8 MB 6.4 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 31.2/99.8 MB 6.2 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 31.4/99.8 MB 6.2 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 31.7/99.8 MB 6.2 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 31.9/99.8 MB 6.2 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 32.2/99.8 MB 6.2 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 32.5/99.8 MB 6.1 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 32.8/99.8 MB 6.1 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 32.8/99.8 MB 5.9 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 33.1/99.8 MB 5.9 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 33.3/99.8 MB 5.8 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 33.3/99.8 MB 5.8 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 33.3/99.8 MB 5.8 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 33.7/99.8 MB 5.5 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 33.9/99.8 MB 5.6 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 34.0/99.8 MB 5.6 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 34.2/99.8 MB 5.5 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 34.4/99.8 MB 5.4 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 34.6/99.8 MB 5.3 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 34.8/99.8 MB 5.2 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 34.9/99.8 MB 5.2 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 35.1/99.8 MB 5.2 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 35.3/99.8 MB 5.1 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 35.5/99.8 MB 5.1 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 35.7/99.8 MB 5.0 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 35.9/99.8 MB 4.9 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 36.0/99.8 MB 4.8 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 36.2/99.8 MB 4.8 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 36.5/99.8 MB 4.7 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 36.6/99.8 MB 4.6 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 36.8/99.8 MB 4.6 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 37.0/99.8 MB 4.6 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 37.1/99.8 MB 4.6 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 37.2/99.8 MB 4.5 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 37.4/99.8 MB 4.5 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 37.6/99.8 MB 4.4 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 37.8/99.8 MB 4.4 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 38.0/99.8 MB 4.3 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 38.1/99.8 MB 4.3 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 38.4/99.8 MB 4.3 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 38.6/99.8 MB 4.3 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 38.8/99.8 MB 4.3 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 39.0/99.8 MB 4.3 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 39.2/99.8 MB 4.2 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 39.4/99.8 MB 4.2 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 39.7/99.8 MB 4.2 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 39.8/99.8 MB 4.2 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 40.0/99.8 MB 4.2 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 40.2/99.8 MB 4.1 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 40.3/99.8 MB 4.1 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 40.6/99.8 MB 4.1 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 40.6/99.8 MB 4.1 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 40.9/99.8 MB 4.1 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 41.2/99.8 MB 4.1 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 41.3/99.8 MB 4.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 41.5/99.8 MB 4.1 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 41.8/99.8 MB 4.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 41.9/99.8 MB 4.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 42.2/99.8 MB 4.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 42.3/99.8 MB 4.0 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 42.5/99.8 MB 4.0 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 4.0 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 3.9 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 43.1/99.8 MB 4.0 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 43.2/99.8 MB 3.9 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 43.4/99.8 MB 4.0 MB/s eta 0:00:15\n",
      "   ----------------- ---------------------- 43.7/99.8 MB 4.2 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 43.8/99.8 MB 4.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 44.0/99.8 MB 4.0 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 44.2/99.8 MB 4.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 44.4/99.8 MB 4.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 44.6/99.8 MB 4.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 44.7/99.8 MB 4.1 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 45.0/99.8 MB 4.1 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 45.2/99.8 MB 4.1 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 45.5/99.8 MB 4.1 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 45.6/99.8 MB 4.1 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 45.8/99.8 MB 4.2 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 46.0/99.8 MB 4.2 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 46.2/99.8 MB 4.2 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 46.4/99.8 MB 4.2 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 46.6/99.8 MB 4.3 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 46.7/99.8 MB 4.2 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 46.9/99.8 MB 4.2 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 47.1/99.8 MB 4.3 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 47.3/99.8 MB 4.3 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 47.5/99.8 MB 4.3 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 47.7/99.8 MB 4.3 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 47.9/99.8 MB 4.3 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 48.2/99.8 MB 4.3 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 48.4/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 48.6/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 48.8/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 48.9/99.8 MB 4.3 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 49.1/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 49.4/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 49.5/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 49.8/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 49.8/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 50.1/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 50.3/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 50.5/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 50.7/99.8 MB 4.4 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 50.9/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 51.2/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 51.4/99.8 MB 4.4 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 51.6/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 51.8/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 52.0/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 52.2/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 52.3/99.8 MB 4.4 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 52.6/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 52.8/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 53.0/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 53.3/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 53.4/99.8 MB 4.6 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 53.6/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 53.8/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 54.1/99.8 MB 4.6 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 54.2/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 54.4/99.8 MB 4.5 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 54.6/99.8 MB 4.5 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 54.8/99.8 MB 4.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 55.1/99.8 MB 4.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 55.4/99.8 MB 4.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 55.7/99.8 MB 4.6 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 55.8/99.8 MB 4.6 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.0/99.8 MB 4.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.3/99.8 MB 4.6 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.5/99.8 MB 4.6 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.7/99.8 MB 4.6 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 56.9/99.8 MB 4.6 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 57.2/99.8 MB 4.7 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 57.4/99.8 MB 4.6 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 57.6/99.8 MB 4.7 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 57.8/99.8 MB 4.7 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 58.0/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 58.2/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 58.4/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 58.6/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 58.8/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 58.9/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 59.2/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 59.3/99.8 MB 4.5 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 59.5/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 59.7/99.8 MB 4.5 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 59.8/99.8 MB 4.5 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 60.1/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 60.2/99.8 MB 4.5 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 60.4/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 60.7/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 60.9/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 61.1/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 61.4/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 61.6/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 61.8/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 62.1/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 62.4/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 62.4/99.8 MB 4.5 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 62.7/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 63.0/99.8 MB 4.6 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 63.2/99.8 MB 4.6 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 63.4/99.8 MB 4.6 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 63.7/99.8 MB 4.6 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 63.9/99.8 MB 4.6 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 64.1/99.8 MB 4.6 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 64.3/99.8 MB 4.6 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 64.5/99.8 MB 4.7 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 64.8/99.8 MB 4.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 65.0/99.8 MB 4.6 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 65.2/99.8 MB 4.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 65.4/99.8 MB 4.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 65.7/99.8 MB 4.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 65.9/99.8 MB 4.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 66.1/99.8 MB 4.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 66.4/99.8 MB 4.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 66.6/99.8 MB 4.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 66.8/99.8 MB 4.6 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 67.1/99.8 MB 4.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 67.2/99.8 MB 4.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 67.4/99.8 MB 4.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 67.6/99.8 MB 4.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 67.8/99.8 MB 4.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 68.0/99.8 MB 4.6 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 68.2/99.8 MB 4.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 68.5/99.8 MB 4.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 68.7/99.8 MB 4.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 69.0/99.8 MB 4.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 69.2/99.8 MB 4.8 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 69.5/99.8 MB 4.9 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 69.7/99.8 MB 5.0 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 70.0/99.8 MB 4.9 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 70.3/99.8 MB 4.9 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 70.6/99.8 MB 4.9 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 70.8/99.8 MB 5.0 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 71.1/99.8 MB 5.0 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 71.3/99.8 MB 5.0 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 71.5/99.8 MB 5.0 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 71.8/99.8 MB 5.0 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 71.9/99.8 MB 4.9 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 72.2/99.8 MB 5.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 72.5/99.8 MB 5.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 72.7/99.8 MB 5.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 73.0/99.8 MB 5.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 73.1/99.8 MB 5.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 73.4/99.8 MB 5.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 73.6/99.8 MB 5.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 73.8/99.8 MB 5.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 74.1/99.8 MB 5.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 74.2/99.8 MB 5.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 74.5/99.8 MB 5.1 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 74.6/99.8 MB 5.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 75.0/99.8 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 75.2/99.8 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 75.5/99.8 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 75.6/99.8 MB 5.0 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 75.9/99.8 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 76.2/99.8 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 76.3/99.8 MB 5.0 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 76.6/99.8 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 76.8/99.8 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 77.1/99.8 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 77.4/99.8 MB 5.2 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 77.5/99.8 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 77.8/99.8 MB 5.2 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 78.1/99.8 MB 5.3 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 78.3/99.8 MB 5.2 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 78.5/99.8 MB 5.2 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 78.9/99.8 MB 5.3 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 79.1/99.8 MB 5.3 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 79.3/99.8 MB 5.2 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 79.6/99.8 MB 5.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 79.9/99.8 MB 5.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 80.1/99.8 MB 5.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 80.4/99.8 MB 5.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 80.8/99.8 MB 5.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 80.8/99.8 MB 5.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 81.1/99.8 MB 5.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 81.4/99.8 MB 5.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 81.7/99.8 MB 5.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 82.0/99.8 MB 5.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 82.4/99.8 MB 5.6 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 82.7/99.8 MB 5.6 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 82.8/99.8 MB 5.6 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 83.2/99.8 MB 5.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 83.4/99.8 MB 5.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 83.7/99.8 MB 5.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 84.0/99.8 MB 5.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 84.0/99.8 MB 5.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 84.5/99.8 MB 5.8 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 84.7/99.8 MB 5.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 85.0/99.8 MB 5.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 85.3/99.8 MB 5.8 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 85.7/99.8 MB 6.0 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 86.1/99.8 MB 6.0 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 86.4/99.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 86.8/99.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 87.1/99.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 87.6/99.8 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.0/99.8 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.2/99.8 MB 6.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.7/99.8 MB 6.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.9/99.8 MB 6.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.3/99.8 MB 6.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.6/99.8 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 90.0/99.8 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 90.4/99.8 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 90.7/99.8 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.0/99.8 MB 6.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.5/99.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 91.6/99.8 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 92.1/99.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 92.5/99.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 92.8/99.8 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.2/99.8 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.6/99.8 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.9/99.8 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.3/99.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.7/99.8 MB 7.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.2/99.8 MB 8.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.6/99.8 MB 8.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.1/99.8 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.5/99.8 MB 8.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 97.0/99.8 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.4/99.8 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.9/99.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.3/99.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.5/99.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.0/99.8 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.4/99.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 99.8/99.8 MB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANGAVI\\AppData\\Local\\Temp\\ipykernel_21336\\305719523.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['status'] = label_encoder.fit_transform(data['status'])\n",
      "C:\\Users\\SANGAVI\\AppData\\Local\\Temp\\ipykernel_21336\\305719523.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = label_encoder.fit_transform(data[feature])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier Accuracy: 0.9351611831173148\n",
      "ExtraTreesClassifier Precision: 0.9533461620697028\n",
      "ExtraTreesClassifier Recall: 0.962857389651896\n",
      "ExtraTreesClassifier F1-score: 0.9580781709963686\n",
      "Confusion Matrix:\n",
      "[[ 5845  1091]\n",
      " [  860 22294]]\n",
      "XGBoostClassifier Accuracy: 0.8993020937188435\n",
      "XGBoostClassifier Precision: 0.9163356504468719\n",
      "XGBoostClassifier Recall: 0.9564654055454781\n",
      "XGBoostClassifier F1-score: 0.9359705845061493\n",
      "Confusion Matrix:\n",
      "[[ 4914  2022]\n",
      " [ 1008 22146]]\n",
      "Logistic Regression Accuracy: 0.7694915254237288\n",
      "Logistic Regression Precision: 0.7694915254237288\n",
      "Logistic Regression Recall: 1.0\n",
      "Logistic Regression F1-score: 0.8697318007662835\n",
      "Confusion Matrix:\n",
      "[[    0  6936]\n",
      " [    0 23154]]\n"
     ]
    }
   ],
   "source": [
    "#Comparing ExtraTreesClassifier, XGBClassifier,LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix\n",
    "\n",
    "# Load your data from a CSV file (replace 'your_data.csv' with your actual file path)\n",
    "df_cleaned = pd.read_csv(\"copper_preprocessed.csv\")\n",
    "data = df_cleaned[df_cleaned['status'].isin(['Won', 'Lost'])]\n",
    "\n",
    "\n",
    "# Select features for prediction (excluding unnecessary ones like 'id' and 'item_date')\n",
    "features = ['quantity tons_log','selling_price_log','application','thickness_log','width','country','customer','product_ref']\n",
    "\n",
    "# Separate target variable (status)\n",
    "target = 'status'\n",
    "label_encoder = LabelEncoder()\n",
    "data['status'] = label_encoder.fit_transform(data['status'])\n",
    "\n",
    "# Handle missing values (replace with your preferred method)\n",
    "# ... (e.g., using imputation techniques)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_features = ['item type']\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_data = pd.DataFrame()\n",
    "for feature in categorical_features:\n",
    "    data[feature] = label_encoder.fit_transform(data[feature])\n",
    "    encoded_data[feature] = data[feature]\n",
    "\n",
    "# Combine numerical and encoded categorical features\n",
    "data_processed = pd.concat([data[features], encoded_data], axis=1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_processed, data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train the ExtraTreesClassifier model\n",
    "model_extra_trees = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "model_extra_trees.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set using ExtraTreesClassifier\n",
    "y_predicted_extra_trees = model_extra_trees.predict(X_test)\n",
    "\n",
    "# Evaluate ExtraTreesClassifier accuracy\n",
    "accuracy_extra_trees = accuracy_score(y_test, y_predicted_extra_trees)\n",
    "precision_extra_trees = precision_score(y_test, y_predicted_extra_trees)\n",
    "recall_extra_trees = recall_score(y_test, y_predicted_extra_trees)  \n",
    "f1_extra_trees = f1_score(y_test, y_predicted_extra_trees)  \n",
    "cm1 = confusion_matrix(y_test, y_predicted_extra_trees)\n",
    "print(\"ExtraTreesClassifier Accuracy:\", accuracy_extra_trees)\n",
    "print(\"ExtraTreesClassifier Precision:\", precision_extra_trees)\n",
    "print(\"ExtraTreesClassifier Recall:\", recall_extra_trees)\n",
    "print(\"ExtraTreesClassifier F1-score:\", f1_extra_trees)\n",
    "print(f\"Confusion Matrix:\\n{cm1}\")\n",
    "\n",
    "# Define and train the XGBoost Classifier model\n",
    "model_xgboost = XGBClassifier(objective='binary:logistic', n_estimators=100, random_state=42)\n",
    "model_xgboost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set using XGBoostClassifier\n",
    "y_predicted_xgboost = model_xgboost.predict(X_test)\n",
    "\n",
    "# Evaluate XGBoostClassifier accuracy\n",
    "accuracy_xgboost = accuracy_score(y_test, y_predicted_xgboost)\n",
    "precision_xgboost = precision_score(y_test, y_predicted_xgboost)\n",
    "recall_xgboost = recall_score(y_test, y_predicted_xgboost)\n",
    "f1_xgboost = f1_score(y_test, y_predicted_xgboost)\n",
    "cm2 = confusion_matrix(y_test, y_predicted_xgboost)\n",
    "print(\"XGBoostClassifier Accuracy:\", accuracy_xgboost)\n",
    "print(\"XGBoostClassifier Precision:\", precision_xgboost)\n",
    "print(\"XGBoostClassifier Recall:\", recall_xgboost)\n",
    "print(\"XGBoostClassifier F1-score:\", f1_xgboost)\n",
    "print(f\"Confusion Matrix:\\n{cm2}\")\n",
    "\n",
    "model_logistic = LogisticRegression(random_state=42)  # Set random state for reproducibility\n",
    "model_logistic.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set using Logistic Regression\n",
    "y_predicted_logistic = model_logistic.predict(X_test)\n",
    "\n",
    "# Evaluate Logistic Regression accuracy\n",
    "accuracy_logistic = accuracy_score(y_test, y_predicted_logistic)\n",
    "precision_logistic = precision_score(y_test, y_predicted_logistic)\n",
    "recall_logistic = recall_score(y_test, y_predicted_logistic)\n",
    "f1_logistic = f1_score(y_test, y_predicted_logistic)\n",
    "cm3 = confusion_matrix(y_test, y_predicted_logistic)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_logistic)\n",
    "print(\"Logistic Regression Precision:\", precision_logistic)\n",
    "print(\"Logistic Regression Recall:\", recall_logistic)\n",
    "print(\"Logistic Regression F1-score:\", f1_logistic)\n",
    "print(f\"Confusion Matrix:\\n{cm3}\")\n",
    "\n",
    "# You can choose the model with the highest accuracy for further analysis or exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANGAVI\\AppData\\Local\\Temp\\ipykernel_21336\\596411381.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['status'] = label_encoder.fit_transform(data['status'])\n",
      "C:\\Users\\SANGAVI\\AppData\\Local\\Temp\\ipykernel_21336\\596411381.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = label_encoder.fit_transform(data[feature])\n",
      "C:\\Users\\SANGAVI\\AppData\\Local\\Temp\\ipykernel_21336\\596411381.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = label_encoder.fit_transform(data[feature])\n",
      "C:\\Users\\SANGAVI\\AppData\\Local\\Temp\\ipykernel_21336\\596411381.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = label_encoder.fit_transform(data[feature])\n",
      "C:\\Users\\SANGAVI\\AppData\\Local\\Temp\\ipykernel_21336\\596411381.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = label_encoder.fit_transform(data[feature])\n",
      "C:\\Users\\SANGAVI\\AppData\\Local\\Temp\\ipykernel_21336\\596411381.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = label_encoder.fit_transform(data[feature])\n",
      "C:\\Users\\SANGAVI\\AppData\\Local\\Temp\\ipykernel_21336\\596411381.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = label_encoder.fit_transform(data[feature])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoostClassifier Accuracy: 0.8950481887670322\n",
      "XGBoostClassifier Precision: 0.9363652234636871\n",
      "XGBoostClassifier Recall: 0.9265785609397944\n",
      "XGBoostClassifier F1-score: 0.9314461859071766\n"
     ]
    }
   ],
   "source": [
    "#XGBClassifier with gridcv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight  import compute_class_weight\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load your data from a CSV file (replace 'your_data.csv' with your actual file path)\n",
    "df_cleaned = pd.read_csv(\"copper_preprocessed.csv\")\n",
    "data = df_cleaned[df_cleaned['status'].isin(['Won', 'Lost'])]\n",
    "\n",
    "# Select features for prediction (excluding unnecessary ones like 'id' and 'item_date')\n",
    "features = ['quantity tons','thickness', 'width']\n",
    "\n",
    "# Separate target variable (status)\n",
    "target = 'status'\n",
    "label_encoder = LabelEncoder()\n",
    "data['status'] = label_encoder.fit_transform(data['status'])\n",
    "\n",
    "# Handle missing values (replace with your preferred method)\n",
    "# ... (e.g., using imputation techniques)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_features = ['customer', 'country', 'item type', 'application', 'material_ref', 'product_ref']\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_data = pd.DataFrame()\n",
    "for feature in categorical_features:\n",
    "    data[feature] = label_encoder.fit_transform(data[feature])\n",
    "    encoded_data[feature] = data[feature]\n",
    "\n",
    "# Combine numerical and encoded categorical features\n",
    "data_processed = pd.concat([data[features], encoded_data], axis=1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_processed, data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weight = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight = dict(zip(np.unique(y_train), class_weight))\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 8],\n",
    "    'gamma': [0, 1, 3],\n",
    "    'colsample_bytree': [0.5, 0.7, 1]\n",
    "}\n",
    "\n",
    "# Define and train the XGBoost Classifier model\n",
    "model_xgboost = XGBClassifier(objective='binary:logistic', n_estimators=100, random_state=42, scale_pos_weight=class_weight[1])\n",
    "grid_search = GridSearchCV(estimator=model_xgboost, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "y_predicted_xgboost = best_model.predict(X_test)\n",
    "\n",
    "# Make predictions on the testing set using XGBoostClassifier\n",
    "# y_predicted_xgboost = model_xgboost.predict(X_test)\n",
    "\n",
    "# Evaluate XGBoostClassifier accuracy\n",
    "accuracy_xgboost = accuracy_score(y_test, y_predicted_xgboost)\n",
    "precision_xgboost = precision_score(y_test, y_predicted_xgboost)\n",
    "recall_xgboost = recall_score(y_test, y_predicted_xgboost)\n",
    "f1_xgboost = f1_score(y_test, y_predicted_xgboost)\n",
    "print(\"XGBoostClassifier Accuracy:\", accuracy_xgboost)\n",
    "print(\"XGBoostClassifier Precision:\", precision_xgboost)\n",
    "print(\"XGBoostClassifier Recall:\", recall_xgboost)\n",
    "print(\"XGBoostClassifier F1-score:\", f1_xgboost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANGAVI\\AppData\\Local\\Temp\\ipykernel_23676\\3662422753.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['status'] = label_encoder_target.fit_transform(data['status'])\n",
      "C:\\Users\\SANGAVI\\AppData\\Local\\Temp\\ipykernel_23676\\3662422753.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[feature] = label_encoders[feature].fit_transform(data[feature])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier Accuracy: 0.9347623795280824\n",
      "ExtraTreesClassifier Precision: 0.9527024140140995\n",
      "ExtraTreesClassifier Recall: 0.9630301459790965\n",
      "ExtraTreesClassifier F1-score: 0.957838441547284\n",
      "Confusion Matrix:\n",
      "[[ 5829  1107]\n",
      " [  856 22298]]\n",
      "Label Encoder for Target Pickled Successfully!\n",
      "ExtraTreesClassifier Model and LabelEncoders Pickled Successfully!\n"
     ]
    }
   ],
   "source": [
    "#Final ExtraTreesClassifier for status prediction \n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix\n",
    "\n",
    "# Load your data from a CSV file (replace 'your_data.csv' with your actual file path)\n",
    "df_cleaned = pd.read_csv(\"copper_preprocessed.csv\")\n",
    "data = df_cleaned[df_cleaned['status'].isin(['Won', 'Lost'])]\n",
    "\n",
    "# Select features for prediction (excluding unnecessary ones like 'id' and 'item_date')\n",
    "features = ['quantity tons_log','selling_price_log','application','thickness_log','width','country','customer','product_ref']\n",
    "\n",
    "# Separate target variable (status)\n",
    "target = 'status'\n",
    "label_encoder_target = LabelEncoder()  # Create a separate LabelEncoder for the target\n",
    "data['status'] = label_encoder_target.fit_transform(data['status'])\n",
    "\n",
    "# Handle missing values (replace with your preferred method)\n",
    "# ... (e.g., using imputation techniques)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_features = ['item type']\n",
    "label_encoders = {}  # Create a dictionary to store LabelEncoders for categorical features\n",
    "for feature in categorical_features:\n",
    "  label_encoders[feature] = LabelEncoder()  # Create a LabelEncoder for each feature\n",
    "  data[feature] = label_encoders[feature].fit_transform(data[feature])\n",
    "\n",
    "# Combine numerical and encoded categorical features\n",
    "data_processed = pd.concat([data[features], data[categorical_features]], axis=1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_processed, data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weight = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight = dict(zip(np.unique(y_train), class_weight))\n",
    "\n",
    "# Define and train the ExtraTreesClassifier model\n",
    "model_et = ExtraTreesClassifier(n_estimators=100, random_state=42, class_weight=class_weight)\n",
    "model_et.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set using ExtraTreesClassifier\n",
    "y_predicted_extra_trees = model_et.predict(X_test)\n",
    "\n",
    "# Evaluate ExtraTreesClassifier accuracy\n",
    "accuracy_extra_trees = accuracy_score(y_test, y_predicted_extra_trees)\n",
    "precision_extra_trees = precision_score(y_test, y_predicted_extra_trees)\n",
    "recall_extra_trees = recall_score(y_test, y_predicted_extra_trees)\n",
    "f1_extra_trees = f1_score(y_test, y_predicted_extra_trees)\n",
    "cm = confusion_matrix(y_test, y_predicted_extra_trees)\n",
    "\n",
    "print(\"ExtraTreesClassifier Accuracy:\", accuracy_extra_trees)\n",
    "print(\"ExtraTreesClassifier Precision:\", precision_extra_trees)\n",
    "print(\"ExtraTreesClassifier Recall:\", recall_extra_trees)\n",
    "print(\"ExtraTreesClassifier F1-score:\", f1_extra_trees)\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "# Pickle the model and label encoders (replace filenames if needed)\n",
    "with open('label_encoder_target.pkl', 'wb') as pickle_file:\n",
    "  pickle.dump(label_encoder_target, pickle_file)\n",
    "\n",
    "print(\"Label Encoder for Target Pickled Successfully!\")\n",
    "with open('extra_trees_model.pkl', 'wb') as pickle_file:\n",
    "  pickle.dump(model_et, pickle_file)\n",
    "\n",
    "with open('label_encoders.pkl', 'wb') as pickle_file:\n",
    "  pickle.dump(label_encoders, pickle_file)\n",
    "\n",
    "print(\"ExtraTreesClassifier Model and LabelEncoders Pickled Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Selling Price: 623.6323023876766\n"
     ]
    }
   ],
   "source": [
    "#Function to predict selling price using random forest model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from math import exp \n",
    "\n",
    "\n",
    "def predict_selling_price(data, model_file=\"random_forest_model.pkl\", scaler_file=\"scaler.pkl\", ohe_file=\"ohe.pkl\"):\n",
    "  \"\"\"\n",
    "  Predicts selling price using a trained Random Forest model and saved scaler and encoder.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): DataFrame containing features for prediction.\n",
    "      model_file (str, optional): Path to the pickled Random Forest model file. Defaults to \"random_forest_model.pkl\".\n",
    "      scaler_file (str, optional): Path to the pickled scaler file. Defaults to \"scaler.pkl\".\n",
    "      ohe_file (str, optional): Path to the pickled OneHotEncoder file. Defaults to \"ohe.pkl\".\n",
    "\n",
    "  Returns:\n",
    "      float: Predicted selling price.\n",
    "  \"\"\"\n",
    "\n",
    "  # Load the model, scaler, and encoder\n",
    "  with open(model_file, \"rb\") as f:\n",
    "      model = pickle.load(f)\n",
    "  with open(scaler_file, \"rb\") as f:\n",
    "      scaler = pickle.load(f)\n",
    "  with open(ohe_file, \"rb\") as f:\n",
    "      ohe = pickle.load(f)\n",
    "\n",
    "  # Extract features from data\n",
    "  selling_price_features = [\n",
    "      \"quantity tons_log\",\n",
    "      \"application\",\n",
    "      \"thickness_log\",\n",
    "      \"width\",\n",
    "      \"country\",\n",
    "      \"customer\",\n",
    "      \"product_ref\",\n",
    "  ]\n",
    "  X = data[selling_price_features]\n",
    "\n",
    "  # Encode categorical features\n",
    "  X_ohe = ohe.transform(data[[\"item type\", \"status\"]]).toarray()\n",
    "\n",
    "  # Combine numerical and encoded features\n",
    "  X = np.concatenate(\n",
    "      (\n",
    "          X[[\n",
    "              \"quantity tons_log\",\n",
    "              \"application\",\n",
    "              \"thickness_log\",\n",
    "              \"width\",\n",
    "              \"country\",\n",
    "              \"customer\",\n",
    "              \"product_ref\",\n",
    "          ]].values,\n",
    "          X_ohe,\n",
    "      ),\n",
    "      axis=1,\n",
    "  )\n",
    "\n",
    "  # Scale the features\n",
    "  X = scaler.transform(X)\n",
    "\n",
    "  # Make prediction\n",
    "  predicted_price = model.predict(X)[0]  # Assuming it returns an array, return the first element\n",
    "\n",
    "   # Convert predicted price from log scale to actual price\n",
    "  actual_selling_price = exp(predicted_price)\n",
    "\n",
    "  return actual_selling_price\n",
    "\n",
    "# Example usage\n",
    "data = pd.DataFrame({\n",
    "  \"quantity tons_log\": [55],\n",
    "  \"application\": [10],\n",
    "  \"thickness_log\": [2],\n",
    "  \"width\": [1200],\n",
    "  \"item type\": ['W'],\n",
    "  \"status\": ['Won'],\n",
    "  \"country\": [28],\n",
    "  \"customer\": [30156308.00],\n",
    "  \"product_ref\": [1670798778],\n",
    "})\n",
    "\n",
    "predicted_price = predict_selling_price(data)\n",
    "print(\"Predicted Selling Price:\", predicted_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Status: Won\n"
     ]
    }
   ],
   "source": [
    "#Function to predict the status using ExtraTreesClassifier\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "def predict_status(data, model_file=\"extra_trees_model.pkl\", label_encoder_target_file=\"label_encoder_target.pkl\", label_encoders_file=\"label_encoders.pkl\"):\n",
    "  \"\"\"\n",
    "  Predicts the status (\"Won\" or \"Lost\") for a new data point using a trained ExtraTreesClassifier model and saved label encoders.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): DataFrame containing features for prediction.\n",
    "      model_file (str, optional): Path to the pickled ExtraTreesClassifier model file. Defaults to \"extra_trees_model.pkl\".\n",
    "      label_encoder_target_file (str, optional): Path to the pickled LabelEncoder file for the target variable. Defaults to \"label_encoder_target.pkl\".\n",
    "      label_encoders_file (str, optional): Path to the pickled dictionary containing LabelEncoders for categorical features. Defaults to \"label_encoders.pkl\".\n",
    "\n",
    "  Returns:\n",
    "      str: Predicted status (\"Won\" or \"Lost\").\n",
    "  \"\"\"\n",
    "\n",
    "  # Load the model and label encoders\n",
    "  with open(model_file, \"rb\") as f:\n",
    "      model = pickle.load(f)\n",
    "  with open(label_encoder_target_file, \"rb\") as f:\n",
    "      label_encoder_target = pickle.load(f)\n",
    "  with open(label_encoders_file, \"rb\") as f:\n",
    "      label_encoders = pickle.load(f)\n",
    "\n",
    "  # Extract features from data\n",
    "  features = ['quantity tons_log', 'selling_price_log',  'application',\n",
    " 'thickness_log', 'width', 'country', 'customer', 'product_ref','item type']\n",
    "  data_processed = data[features]\n",
    "\n",
    "  # Encode categorical features\n",
    "  categorical_features = ['item type']\n",
    "  for feature in categorical_features:\n",
    "      data_processed[feature] = label_encoders[feature].transform(data_processed[feature])\n",
    "\n",
    "  # Make prediction\n",
    "  predict_status = model.predict(data_processed)\n",
    "#   print(predict_status.shape)\n",
    "  predicted_status_decoded = label_encoder_target.inverse_transform(predict_status)\n",
    "\n",
    "\n",
    "  return predicted_status_decoded\n",
    "\n",
    "# Example usage\n",
    "data = pd.DataFrame({\n",
    "  \"quantity tons_log\": [55],\n",
    "  \"selling_price_log\": [3.5],\n",
    "  \"application\": [10],\n",
    "  \"thickness_log\": [0.8],\n",
    "  \"width\": [1200],\n",
    "  \"country\": [28],\n",
    "  \"customer\": [30156308.00],\n",
    "  \"product_ref\": [1670798778],\n",
    "  \"item type\": ['IPL']\n",
    "})\n",
    "\n",
    "predicted_status = predict_status(data)\n",
    "print(\"Predicted Status:\", predicted_status[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
